"""
EFT AI ì—”ì§„ - Llama 3 ê¸°ë°˜ ì‹¬ë¦¬ìƒë‹´ íŠ¹í™” AI
transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•œ ë¡œì»¬/í´ë¼ìš°ë“œ LLM ì¶”ë¡ 
"""

import torch
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer, 
    BitsAndBytesConfig,
    pipeline
)
from typing import Optional, Dict, Any, List, AsyncGenerator
import asyncio
import time
from datetime import datetime
import json
import gc
import psutil
import GPUtil

from config.settings import get_settings
from utils.logger import get_logger
from models.chat_models import EmotionAnalysis, ModelStats

logger = get_logger(__name__)
settings = get_settings()

class EFTAIEngine:
    """EFT ì „ë¬¸ AI ì—”ì§„"""
    
    def __init__(
        self, 
        model_name: str = None,
        device: str = "auto",
        max_memory: str = None
    ):
        self.model_name = model_name or settings.MODEL_NAME
        self.device = device if device != "auto" else self._detect_best_device()
        self.max_memory = max_memory or settings.MAX_MEMORY
        
        # ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € (ì´ˆê¸°í™” í›„ ë¡œë“œ)
        self.model = None
        self.tokenizer = None
        self.generation_pipeline = None
        
        # ì„±ëŠ¥ í†µê³„
        self.stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "total_processing_time": 0.0,
            "start_time": time.time(),
            "errors": []
        }
        
        logger.info(f"EFT AI Engine ì´ˆê¸°í™”: {self.model_name} on {self.device}")
    
    def _detect_best_device(self) -> str:
        """ìµœì  ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€"""
        if torch.cuda.is_available():
            gpu_count = torch.cuda.device_count()
            logger.info(f"CUDA ì‚¬ìš© ê°€ëŠ¥, GPU {gpu_count}ê°œ ê°ì§€")
            return "cuda"
        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            logger.info("Apple Silicon MPS ì‚¬ìš©")
            return "mps"
        else:
            logger.info("CPU ëª¨ë“œ ì‚¬ìš©")
            return "cpu"
    
    def _setup_quantization_config(self) -> Optional[BitsAndBytesConfig]:
        """ì–‘ìí™” ì„¤ì • (ë©”ëª¨ë¦¬ ì ˆì•½ìš©) - bitsandbytes íŒ¨í‚¤ì§€ ì—†ì´ ë¹„í™œì„±í™”"""
        logger.info("ì–‘ìí™” ë¹„í™œì„±í™” (bitsandbytes íŒ¨í‚¤ì§€ ë¶ˆí•„ìš”)")
        return None
    
    async def initialize(self) -> None:
        """ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ"""
        try:
            logger.info(f"ğŸ¤– ëª¨ë¸ ë¡œë“œ ì‹œì‘: {self.model_name}")
            start_time = time.time()
            
            # 1. í† í¬ë‚˜ì´ì € ë¡œë“œ
            logger.info("ğŸ“ í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                cache_dir=settings.MODEL_CACHE_DIR,
                token=settings.HUGGINGFACE_TOKEN
            )
            
            # íŒ¨ë”© í† í° ì„¤ì • (LlamaëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ì—†ìŒ)
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # 2. ì–‘ìí™” ì„¤ì •
            quantization_config = self._setup_quantization_config()
            
            # 3. ëª¨ë¸ ë¡œë“œ
            logger.info("ğŸ§  ì–¸ì–´ëª¨ë¸ ë¡œë“œ ì¤‘... (ìˆ˜ ë¶„ ì†Œìš” ê°€ëŠ¥)")
            
            model_kwargs = {
                "cache_dir": settings.MODEL_CACHE_DIR,
                "torch_dtype": torch.float16 if self.device == "cuda" else torch.float32,
                "device_map": "auto" if self.device == "cuda" else None,
                "token": settings.HUGGINGFACE_TOKEN
            }
            
            if quantization_config:
                model_kwargs["quantization_config"] = quantization_config
            
            if self.max_memory:
                model_kwargs["max_memory"] = {0: self.max_memory}
            
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                **model_kwargs
            )
            
            # CPU ëª¨ë“œì—ì„œëŠ” ì§ì ‘ ë””ë°”ì´ìŠ¤ ì´ë™
            if self.device == "cpu":
                self.model = self.model.to(self.device)
            
            # 4. ìƒì„± íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
            logger.info("âš¡ ìƒì„± íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì¤‘...")
            self.generation_pipeline = pipeline(
                "text-generation",
                model=self.model,
                tokenizer=self.tokenizer,
                device=0 if self.device == "cuda" else -1,
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                do_sample=True,
                return_full_text=False
            )
            
            load_time = time.time() - start_time
            logger.info(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ! ({load_time:.1f}ì´ˆ ì†Œìš”)")
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¡œê¹…
            self._log_memory_usage()
            
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise e
    
    def _log_memory_usage(self):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¡œê¹…"""
        try:
            # RAM ì‚¬ìš©ëŸ‰
            ram = psutil.virtual_memory()
            logger.info(f"ğŸ’¾ RAM ì‚¬ìš©ëŸ‰: {ram.used / 1024**3:.1f}GB / {ram.total / 1024**3:.1f}GB")
            
            # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (CUDA ì‚¬ìš© ì‹œ)
            if self.device == "cuda" and torch.cuda.is_available():
                for i in range(torch.cuda.device_count()):
                    memory_allocated = torch.cuda.memory_allocated(i) / 1024**3
                    memory_reserved = torch.cuda.memory_reserved(i) / 1024**3
                    logger.info(f"ğŸ® GPU {i} ë©”ëª¨ë¦¬: {memory_allocated:.1f}GB allocated, {memory_reserved:.1f}GB reserved")
            
        except Exception as e:
            logger.warning(f"ë©”ëª¨ë¦¬ ë¡œê¹… ì‹¤íŒ¨: {e}")
    
    async def generate_response(
        self,
        prompt: str,
        max_tokens: int = 400,
        temperature: float = 0.7,
        top_p: float = 0.9,
        top_k: int = 50
    ) -> str:
        """AI ì‘ë‹µ ìƒì„± (ë‹¨ì¼ ì‘ë‹µ)"""
        
        if not self.model or not self.tokenizer:
            raise RuntimeError("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. initialize()ë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")
        
        self.stats["total_requests"] += 1
        start_time = time.time()
        
        try:
            # ë¹„ë™ê¸° ì²˜ë¦¬ë¥¼ ìœ„í•´ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None, 
                self._generate_sync, 
                prompt, max_tokens, temperature, top_p, top_k
            )
            
            processing_time = time.time() - start_time
            self.stats["total_processing_time"] += processing_time
            self.stats["successful_requests"] += 1
            
            logger.info(f"âœ… ì‘ë‹µ ìƒì„± ì™„ë£Œ ({processing_time:.2f}ì´ˆ)")
            return response
            
        except Exception as e:
            error_msg = f"ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {str(e)}"
            logger.error(error_msg)
            self.stats["errors"].append({
                "timestamp": datetime.now().isoformat(),
                "error": error_msg
            })
            raise e
    
    def _generate_sync(
        self, 
        prompt: str, 
        max_tokens: int, 
        temperature: float, 
        top_p: float, 
        top_k: int
    ) -> str:
        """ë™ê¸°ì  í…ìŠ¤íŠ¸ ìƒì„± (ë‚´ë¶€ ë©”ì„œë“œ)"""
        
        try:
            # ëª¨ë¸ë³„ í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ… (DialoGPT vs Llama êµ¬ë¶„)
            if "DialoGPT" in self.model_name:
                formatted_prompt = self._format_dialogpt_prompt(prompt)
                # DialoGPT í† í° ê¸¸ì´ ì œí•œ (ë” ë³´ìˆ˜ì ìœ¼ë¡œ ì„¤ì •)
                max_input_length = 200  # ë§¤ìš° ì§§ê²Œ ì„¤ì •
                safe_max_tokens = min(max_tokens, 100)  # ì•ˆì „í•œ ì¶œë ¥ ê¸¸ì´
            else:
                formatted_prompt = self._format_llama_prompt(prompt)
                max_input_length = 4000  # Llama ëª¨ë¸ì€ ë” ì—¬ìœ ë¡­ê²Œ
                safe_max_tokens = max_tokens
            
            # ì…ë ¥ í† í° ê¸¸ì´ ì²´í¬ ë° ì œí•œ
            input_tokens = self.tokenizer.encode(formatted_prompt, return_tensors="pt")
            
            if input_tokens.shape[1] > max_input_length:
                logger.warning(f"ì…ë ¥ í† í° ê¸¸ì´ ì´ˆê³¼ ({input_tokens.shape[1]} > {max_input_length}), ìë¥´ê¸° ì ìš©")
                # ë’¤ì—ì„œë¶€í„° ìë¥´ê¸° (ìµœê·¼ ëŒ€í™” ìœ ì§€)
                truncated_tokens = input_tokens[:, -max_input_length:]
                formatted_prompt = self.tokenizer.decode(truncated_tokens[0], skip_special_tokens=True)
                logger.info(f"í† í° ê¸¸ì´ ì¡°ì •: {input_tokens.shape[1]} â†’ {max_input_length}")
            
            # ìƒì„± íŒŒë¼ë¯¸í„°
            generation_params = {
                "max_new_tokens": safe_max_tokens,
                "temperature": temperature,
                "top_p": top_p,
                "top_k": top_k,
                "do_sample": True,
                "pad_token_id": self.tokenizer.eos_token_id,
                "eos_token_id": self.tokenizer.eos_token_id,
                "truncation": True
            }
            
            # DialoGPT ì „ìš© íŒŒë¼ë¯¸í„° ì¶”ê°€
            if "DialoGPT" in self.model_name:
                generation_params["max_length"] = 1024  # ì „ì²´ ê¸¸ì´ ì œí•œ
            
            # í…ìŠ¤íŠ¸ ìƒì„±
            outputs = self.generation_pipeline(
                formatted_prompt,
                **generation_params
            )
            
            # ì‘ë‹µ ì¶”ì¶œ ë° í›„ì²˜ë¦¬
            generated_text = outputs[0]["generated_text"]
            logger.info(f"ğŸ¤– DialoGPT ì›ë³¸ ì¶œë ¥: {repr(generated_text)}")
            
            cleaned_response = self._clean_response(generated_text, formatted_prompt)
            
            return cleaned_response
            
        except Exception as e:
            logger.error(f"ë™ê¸° ìƒì„± ì‹¤íŒ¨: {e}")
            raise e
    
    def _format_llama_prompt(self, user_prompt: str) -> str:
        """Llama ëª¨ë¸ìš© í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ…"""
        
        # Llama-2/3 Chat í…œí”Œë¦¿ ì ìš©
        if "llama-2" in self.model_name.lower():
            formatted = f"<s>[INST] {user_prompt} [/INST]"
        elif "llama-3" in self.model_name.lower():
            formatted = f"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\n{user_prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n"
        else:
            # ê¸°ë³¸ í¬ë§·
            formatted = f"Human: {user_prompt}\n\nAssistant: "
        
        return formatted
    
    def _format_dialogpt_prompt(self, user_prompt: str) -> str:
        """DialoGPT ëª¨ë¸ìš© í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ… (ê°„ë‹¨í•œ ëŒ€í™”í˜•)"""
        
        # DialoGPTë¥¼ ìœ„í•œ ê°„ë‹¨í•˜ì§€ë§Œ ëª…í™•í•œ EFT ìƒë‹´ì‚¬ ì„¤ì •
        formatted = f"User: {user_prompt}{self.tokenizer.eos_token}EFT Counselor:"
        
        return formatted
    
    def _clean_response(self, generated_text: str, prompt: str) -> str:
        """ì‘ë‹µ í›„ì²˜ë¦¬ ë° ì •ë¦¬"""
        
        # í”„ë¡¬í”„íŠ¸ ì œê±°
        cleaned = generated_text
        
        # íŠ¹ìˆ˜ í† í° ì œê±°
        special_tokens = [
            "<|eot_id|>", "<|end_of_text|>", "</s>", 
            "<|start_header_id|>", "<|end_header_id|>",
            "[INST]", "[/INST]", "<s>"
        ]
        
        for token in special_tokens:
            cleaned = cleaned.replace(token, "")
        
        # ê³µë°± ì •ë¦¬
        cleaned = cleaned.strip()
        
        # ë„ˆë¬´ ê¸´ ì‘ë‹µ ìë¥´ê¸°
        if len(cleaned) > 1500:
            sentences = cleaned.split('. ')
            cleaned = '. '.join(sentences[:5]) + '.'
        
        # ë””ë²„ê¹…ì„ ìœ„í•œ ë¡œê·¸ ì¶”ê°€
        logger.info(f"ğŸ” ìƒì„±ëœ ì›ë³¸ í…ìŠ¤íŠ¸: {repr(generated_text)}")
        logger.info(f"ğŸ” ì •ì œëœ í…ìŠ¤íŠ¸: {repr(cleaned)}")
        
        # ë¹ˆ ì‘ë‹µ ì²˜ë¦¬
        if not cleaned:
            cleaned = "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•˜ëŠ”ë° ë¬¸ì œê°€ ìˆì—ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ë§ì”€í•´ ì£¼ì‹œê² ì–´ìš”?"
        
        return cleaned
    
    async def generate_stream(
        self, 
        message: str, 
        emotion_state: EmotionAnalysis
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„± (ê¸´ ì‘ë‹µìš©)"""
        
        # TODO: ì‹¤ì œ ìŠ¤íŠ¸ë¦¬ë° êµ¬í˜„
        # í˜„ì¬ëŠ” ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ ì‹œë®¬ë ˆì´ì…˜
        
        response = await self.generate_response(message)
        chunks = self._split_into_chunks(response, chunk_size=50)
        
        for i, chunk in enumerate(chunks):
            yield {
                "chunk_type": "text",
                "content": chunk,
                "sequence_number": i,
                "is_final": i == len(chunks) - 1
            }
            
            # ìŠ¤íŠ¸ë¦¬ë° ì‹œë®¬ë ˆì´ì…˜ì„ ìœ„í•œ ì§€ì—°
            await asyncio.sleep(0.1)
    
    def _split_into_chunks(self, text: str, chunk_size: int = 50) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• """
        words = text.split()
        chunks = []
        
        for i in range(0, len(words), chunk_size):
            chunk = ' '.join(words[i:i + chunk_size])
            chunks.append(chunk)
        
        return chunks
    
    async def get_performance_stats(self) -> ModelStats:
        """ëª¨ë¸ ì„±ëŠ¥ í†µê³„ ë°˜í™˜"""
        
        uptime = time.time() - self.stats["start_time"]
        avg_response_time = (
            self.stats["total_processing_time"] / max(self.stats["successful_requests"], 1)
        )
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê³„ì‚°
        memory_usage = 0.0
        gpu_utilization = None
        
        try:
            if self.device == "cuda" and torch.cuda.is_available():
                memory_usage = torch.cuda.memory_allocated(0) / 1024**3
                
                # GPU ì‚¬ìš©ë¥  (ì˜µì…˜)
                try:
                    gpus = GPUtil.getGPUs()
                    if gpus:
                        gpu_utilization = gpus[0].load
                except:
                    pass
            else:
                # CPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì •
                process = psutil.Process()
                memory_usage = process.memory_info().rss / 1024**3
                
        except Exception as e:
            logger.warning(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê³„ì‚° ì‹¤íŒ¨: {e}")
        
        return ModelStats(
            model_name=self.model_name,
            total_requests=self.stats["total_requests"],
            successful_requests=self.stats["successful_requests"],
            average_response_time=avg_response_time,
            memory_usage_gb=memory_usage,
            gpu_utilization=gpu_utilization,
            uptime_hours=uptime / 3600,
            last_updated=datetime.now().isoformat()
        )
    
    async def cleanup(self) -> None:
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        logger.info("ğŸ”„ AI ì—”ì§„ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘...")
        
        try:
            if self.model:
                del self.model
                self.model = None
            
            if self.tokenizer:
                del self.tokenizer
                self.tokenizer = None
                
            if self.generation_pipeline:
                del self.generation_pipeline
                self.generation_pipeline = None
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            gc.collect()
            
            if self.device == "cuda" and torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            logger.info("âœ… ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {e}")

# ì „ì—­ AI ì—”ì§„ ì¸ìŠ¤í„´ìŠ¤ (ì‹±ê¸€í†¤)
_ai_engine_instance: Optional[EFTAIEngine] = None

def get_ai_engine() -> EFTAIEngine:
    """AI ì—”ì§„ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ì‹±ê¸€í†¤)"""
    global _ai_engine_instance
    if _ai_engine_instance is None:
        _ai_engine_instance = EFTAIEngine()
    return _ai_engine_instance