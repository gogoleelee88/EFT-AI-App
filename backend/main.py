"""
EFT AI ì„œë²„ - FastAPI ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜
ì‹¬ë¦¬ìƒë‹´ íŠ¹í™” Llama 3 ê¸°ë°˜ AI ì„œë²„
"""

from fastapi import FastAPI, HTTPException, BackgroundTasks, Request, Header
from fastapi.middleware.cors import CORSMiddleware
from starlette.middleware.base import BaseHTTPMiddleware
from starlette.types import ASGIApp, Receive, Scope, Send
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
import asyncio
import time
from datetime import datetime
import json
import os
from pathlib import Path
import itertools
import random
import logging
import uuid
from collections import defaultdict, deque

# ë¡œì»¬ ëª¨ë“ˆ ì„í¬íŠ¸
from services.ai_engine import EFTAIEngine
from services.prompt_manager import EFTPromptManager
from services.emotion_analyzer import EmotionAnalyzer
from models.chat_models import ChatRequest, ChatResponse, StreamResponse
from config.settings import get_settings
from utils.logger import get_logger

# ì„¤ì • ë° ë¡œê±°
settings = get_settings()
logger = get_logger(__name__)

# --- A/B ë¼ìš°íŒ… ìƒíƒœ ---
_engine_cycle = None
_engine_keys = list(settings.FREE_ENGINES.keys())

def _init_cycle():
    global _engine_cycle
    _engine_cycle = itertools.cycle(_engine_keys)

_init_cycle()

def pick_engine(strategy: str, user_id: Optional[str] = None):
    """ì „ëµì— ë”°ë¼ A/B ì—”ì§„ ì„ íƒ (4ê°€ì§€ ì „ëµ ì§€ì›)"""
    if strategy == "random":
        return random.choice(_engine_keys)
    elif strategy == "weighted":
        # weighted ì˜ˆ: FREE_ENGINES_WEIGHTS="engine_a:2,engine_b:1"
        wstr = os.getenv("FREE_ENGINES_WEIGHTS", "")
        weights = []
        keys = []
        for token in filter(None, (t.strip() for t in wstr.split(","))):
            k, _, w = token.partition(":")
            if k in settings.FREE_ENGINES and w.isdigit():
                keys.append(k); weights.append(int(w))
        if keys and weights:
            return random.choices(keys, weights=weights, k=1)[0]
        # fallback
        return random.choice(_engine_keys)
    elif strategy == "sticky" and user_id:
        # ì‚¬ìš©ìë³„ ê³ ì • ì—”ì§„ (ë™ì¼ ì‚¬ìš©ìëŠ” í•­ìƒ ê°™ì€ ì—”ì§„)
        if user_id in settings.STICKY_SESSIONS:
            engine_key = settings.STICKY_SESSIONS[user_id]
            if engine_key in settings.FREE_ENGINES:
                return engine_key
        # ìƒˆ ì‚¬ìš©ìëŠ” ëœë¤ ë°°ì •
        engine_key = random.choice(_engine_keys)
        settings.STICKY_SESSIONS[user_id] = engine_key
        logger.info(f"[STICKY] ìƒˆ ì‚¬ìš©ì {user_id} -> {engine_key} ë§¤í•‘")
        return engine_key
    # default: round_robin
    return next(_engine_cycle)

class ABRouteMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: Request, call_next):
        # 1) ì‚¬ìš©ì í‹°ì–´ ê²°ì •: í—¤ë”ë¡œ ê°•ì œ ê°€ëŠ¥ (x-user-tier), ê¸°ë³¸ì€ settings.USER_TIER
        user_tier = request.headers.get("x-user-tier", settings.USER_TIER).lower()

        # 2) ë¬´ë£Œ í‹°ì–´ì¼ ë•Œë§Œ A/B ì—”ì§„ ì„ íƒ (ì¿¼ë¦¬ë‚˜ í—¤ë”ë¡œ override ê°€ëŠ¥)
        if user_tier == "free":
            # override: x-free-engine: engine_a|engine_b
            forced = request.headers.get("x-free-engine")
            if forced and forced in settings.FREE_ENGINES:
                engine_key = forced
            else:
                # user_id ì¶”ì¶œ (í—¤ë”, ì¿¼ë¦¬, ë˜ëŠ” ì„¸ì…˜ì—ì„œ)
                user_id = request.headers.get("x-user-id") or request.query_params.get("user_id")
                engine_key = pick_engine(settings.AB_TEST_STRATEGY, user_id)
            request.state.free_engine_key = engine_key
            request.state.free_engine = settings.FREE_ENGINES[engine_key]
            logger.info(f"[A/B] user_tier=free -> {engine_key} ({request.state.free_engine['model']})")
        else:
            request.state.free_engine_key = None
            request.state.free_engine = None

        response = await call_next(request)
        # ì‘ë‹µ í—¤ë”ì— ì–´ë–¤ ì—”ì§„ì´ ì“°ì˜€ëŠ”ì§€ ë…¸ì¶œ(ê´€ì¸¡ì„±)
        if user_tier == "free" and request.state.free_engine_key:
            response.headers["x-ab-engine"] = request.state.free_engine_key
        return response

# FastAPI ì•± ì´ˆê¸°í™”
app = FastAPI(
    title="EFT AI ìƒë‹´ ì„œë²„",
    description="EFT(ê°ì •ììœ ê¸°ë²•) ì „ë¬¸ AI ìƒë‹´ ì„œë¹„ìŠ¤",
    version="1.0.0",
    docs_url="/docs" if settings.DEBUG else None,
    redoc_url="/redoc" if settings.DEBUG else None
)

# CORS ë„ë©”ì¸ í™˜ê²½ë³€ìˆ˜ ë³‘í•©
extra = (settings.EXTRA_ALLOWED_ORIGINS or "").strip()
if extra:
    settings.ALLOWED_ORIGINS.extend([o.strip() for o in extra.split(",") if o.strip()])
    logger.info(f"ì¶”ê°€ CORS ë„ë©”ì¸ ë“±ë¡: {extra}")

# === í”„ë¡œë•ì…˜ ë³´ì•ˆ ë¯¸ë“¤ì›¨ì–´ ì¶”ê°€ ===

# 1. ìš”ì²­ ë°”ë”” í¬ê¸° ì œí•œ (DoS ë°©ì§€)
class MaxBodySizeMiddleware(BaseHTTPMiddleware):
    def __init__(self, app: ASGIApp, max_bytes: int = 128 * 1024):
        super().__init__(app)
        self.max_bytes = max_bytes

    async def dispatch(self, request: Request, call_next):
        cl = request.headers.get("content-length")
        if cl and cl.isdigit() and int(cl) > self.max_bytes:
            logger.warning(f"ìš”ì²­ í¬ê¸° ì´ˆê³¼: {cl} bytes (ìµœëŒ€: {self.max_bytes})")
            raise HTTPException(status_code=413, detail="Payload too large")
        return await call_next(request)

# 2. ìƒê´€ê´€ê³„ ID ì¶”ì  (ë””ë²„ê¹… ìš©ì´)
class CorrelationIdMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: Request, call_next):
        cid = request.headers.get("x-request-id") or uuid.uuid4().hex
        request.state.correlation_id = cid
        response = await call_next(request)
        response.headers["x-request-id"] = cid
        return response

# 3. ê°„ë‹¨í•œ ë ˆì´íŠ¸ ë¦¬ë°‹ (ë©”ëª¨ë¦¬ ê¸°ë°˜)
class SimpleRateLimitMiddleware(BaseHTTPMiddleware):
    def __init__(self, app: ASGIApp, requests_per_minute: int = 60):
        super().__init__(app)
        self.requests_per_minute = requests_per_minute
        self.request_times = defaultdict(deque)  # IP -> deque of timestamps
        
    async def dispatch(self, request: Request, call_next):
        # ë¬´ë£Œ í‹°ì–´ API ê²½ë¡œì—ë§Œ ì ìš©
        if not request.url.path.startswith("/api/chat"):
            return await call_next(request)
            
        client_ip = request.client.host if request.client else "unknown"
        current_time = time.time()
        
        # 1ë¶„ ì´ì „ ìš”ì²­ë“¤ ì œê±°
        client_requests = self.request_times[client_ip]
        while client_requests and current_time - client_requests[0] > 60:
            client_requests.popleft()
            
        # ë ˆì´íŠ¸ ë¦¬ë°‹ ì²´í¬
        if len(client_requests) >= self.requests_per_minute:
            logger.warning(f"ë ˆì´íŠ¸ ë¦¬ë°‹ ì´ˆê³¼: {client_ip} ({len(client_requests)} requests/min)")
            raise HTTPException(
                status_code=429, 
                detail=f"Rate limit exceeded. Maximum {self.requests_per_minute} requests per minute.",
                headers={"Retry-After": "60"}
            )
            
        # í˜„ì¬ ìš”ì²­ ê¸°ë¡
        client_requests.append(current_time)
        return await call_next(request)

# ë¯¸ë“¤ì›¨ì–´ ë“±ë¡ (ìˆœì„œ ì¤‘ìš”!)
app.add_middleware(MaxBodySizeMiddleware, max_bytes=256 * 1024)  # 256KBë¡œ ì—¬ìœ  ìˆê²Œ
app.add_middleware(CorrelationIdMiddleware)
app.add_middleware(SimpleRateLimitMiddleware, requests_per_minute=120)  # ë¶„ë‹¹ 120íšŒ
app.add_middleware(ABRouteMiddleware)

# CORS ì„¤ì • (PWA í´ë¼ì´ì–¸íŠ¸ ì—°ê²°ìš©)
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.ALLOWED_ORIGINS,
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE"],
    allow_headers=["*"],
)

# AI ì—”ì§„ ì „ì—­ ë³€ìˆ˜ (ì„œë²„ ì‹œì‘ì‹œ ë¡œë“œ)
ai_engine: Optional[EFTAIEngine] = None  # ë¬´ë£Œ ëª¨ë¸
premium_ai_engine: Optional[EFTAIEngine] = None  # í”„ë¦¬ë¯¸ì—„ ëª¨ë¸
prompt_manager: Optional[EFTPromptManager] = None
emotion_analyzer: Optional[EmotionAnalyzer] = None

@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ì‹œ AI ëª¨ë¸ ë¡œë“œ"""
    global ai_engine, premium_ai_engine, prompt_manager, emotion_analyzer
    
    logger.info("ğŸš€ EFT AI ì„œë²„ ì‹œì‘ ì¤‘...")
    
    try:
        # 1. í”„ë¡¬í”„íŠ¸ ë§¤ë‹ˆì € ì´ˆê¸°í™”
        logger.info("ğŸ“ í”„ë¡¬í”„íŠ¸ ì‹œìŠ¤í…œ ë¡œë“œ ì¤‘...")
        prompt_manager = EFTPromptManager()
        
        # 2. ê°ì • ë¶„ì„ê¸° ì´ˆê¸°í™”
        logger.info("ğŸ§  ê°ì • ë¶„ì„ ì‹œìŠ¤í…œ ë¡œë“œ ì¤‘...")
        emotion_analyzer = EmotionAnalyzer()
        
        logger.info("âœ… ê¸°ë³¸ ì„œë¹„ìŠ¤ ì‹œì‘ ì™„ë£Œ!")
        logger.info("ğŸ’¡ AI ëª¨ë¸ì€ vLLM ì„œë²„ ì—°ë™ì„ í†µí•´ ì œê³µë©ë‹ˆë‹¤")
        
        # AI ì—”ì§„ ë¡œë“œëŠ” ì„ íƒì‚¬í•­ìœ¼ë¡œ ë³€ê²½ (PyTorch ì´ìŠˆ ìš°íšŒ)
        try:
            # 3. ë¬´ë£Œ AI ì—”ì§„ ì´ˆê¸°í™” (DialoGPT) - ì„ íƒì‚¬í•­
            logger.info("ğŸ†“ ë¡œì»¬ AI ëª¨ë¸ ë¡œë“œ ì‹œë„ ì¤‘...")
            ai_engine = EFTAIEngine(
                model_name=settings.FREE_TIER_MODEL,
                device=settings.DEVICE,
                max_memory=settings.MAX_MEMORY
            )
            await ai_engine.initialize()
            logger.info("âœ… ë¡œì»¬ AI ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
        except Exception as ai_error:
            logger.warning(f"âš ï¸ ë¡œì»¬ AI ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {ai_error}")
            logger.info("ğŸ“¢ vLLM ì„œë²„ ì—°ë™ìœ¼ë¡œ ëŒ€ì²´ ê°€ëŠ¥í•©ë‹ˆë‹¤")
            ai_engine = None
        
        # 4. í”„ë¦¬ë¯¸ì—„ ëª¨ë¸ë„ ì„ íƒì‚¬í•­
        if ai_engine:  # ê¸°ë³¸ ëª¨ë¸ì´ ìˆì„ ë•Œë§Œ ì‹œë„
            try:
                logger.info("ğŸ’ í”„ë¦¬ë¯¸ì—„ ëª¨ë¸ ë¡œë“œ ì‹œë„ ì¤‘...")
                premium_ai_engine = EFTAIEngine(
                    model_name=settings.PREMIUM_TIER_MODEL,
                    device=settings.DEVICE,
                    max_memory=settings.MAX_MEMORY
                )
                await premium_ai_engine.initialize()
                logger.info("âœ… í”„ë¦¬ë¯¸ì—„ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
            except Exception as premium_error:
                logger.warning(f"âš ï¸ í”„ë¦¬ë¯¸ì—„ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {premium_error}")
                premium_ai_engine = None
        
        logger.info("ğŸš€ EFT AI ì„œë²„ ì™„ì „íˆ ì‹œì‘ ì™„ë£Œ!")
        
    except Exception as e:
        logger.error(f"âŒ ì¤‘ìš”í•œ ì„œë¹„ìŠ¤ ì‹œì‘ ì‹¤íŒ¨: {e}")
        # AI ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ëŠ” í—ˆìš©, ê¸°ë³¸ ì„œë¹„ìŠ¤ë§Œìœ¼ë¡œë„ ì„œë²„ ì‹œì‘
        logger.info("ğŸ“¢ vLLM ì—°ë™ ëª¨ë“œë¡œ ì„œë²„ ê³„ì† ì‹¤í–‰í•©ë‹ˆë‹¤")

@app.on_event("shutdown")
async def shutdown_event():
    """ì„œë²„ ì¢…ë£Œì‹œ ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
    logger.info("ğŸ”„ ì„œë²„ ì¢…ë£Œ ì¤‘...")
    
    if ai_engine:
        await ai_engine.cleanup()
    
    if premium_ai_engine:
        await premium_ai_engine.cleanup()
        
    logger.info("âœ… ì„œë²„ ì¢…ë£Œ ì™„ë£Œ")

# ê¸°ë³¸ ì—”ë“œí¬ì¸íŠ¸
@app.get("/")
async def root():
    """ì„œë²„ ìƒíƒœ í™•ì¸"""
    return {
        "service": "EFT AI ìƒë‹´ ì„œë²„",
        "status": "running",
        "version": "1.0.0",
        "model_loaded": ai_engine is not None,
        "timestamp": datetime.now().isoformat()
    }

@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸ (í–¥ìƒëœ ê´€ì¸¡ì„±)"""
    return {
        "status": "healthy",
        "tier": settings.USER_TIER,
        "strategy": settings.AB_TEST_STRATEGY,
        "free_engines": {k: {"model": v["model"], "port": v["port"]} for k, v in settings.FREE_ENGINES.items()},
        "free_ai_engine": "loaded" if ai_engine else "not_loaded",
        "premium_ai_engine": "loaded" if premium_ai_engine else "not_loaded",
        "prompt_manager": "loaded" if prompt_manager else "not_loaded",
        "emotion_analyzer": "loaded" if emotion_analyzer else "not_loaded",
        "uptime": time.time(),
        "available_tiers": ["free", "premium"],  # í”„ë¦¬ë¯¸ì—„ì€ í•­ìƒ ì‚¬ìš© ê°€ëŠ¥ (í´ë°± ì§€ì›)
        "sticky_sessions_count": len(getattr(settings, 'STICKY_SESSIONS', {})),
        "supported_strategies": ["round_robin", "random", "weighted", "sticky"],
        "vllm_timeouts": {
            "connect": getattr(settings, 'VLLM_CONNECT_TIMEOUT', 10.0),
            "read": getattr(settings, 'VLLM_READ_TIMEOUT', 120.0),
            "health_check": getattr(settings, 'VLLM_HEALTH_CHECK_TIMEOUT', 5.0)
        },
        "memory_usage": "TODO: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰"
    }

@app.get("/api/health")
async def health_check_api():
    """í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸ (API ê²½ë¡œ)"""
    # ë™ì¼í•œ ì‘ë‹µ ë°˜í™˜
    return {
        "status": "healthy",
        "free_ai_engine": "loaded" if ai_engine else "not_loaded",
        "premium_ai_engine": "loaded" if premium_ai_engine else "not_loaded",
        "prompt_manager": "loaded" if prompt_manager else "not_loaded",
        "emotion_analyzer": "loaded" if emotion_analyzer else "not_loaded",
        "uptime": time.time(),
        "available_tiers": ["free", "premium"],  # í”„ë¦¬ë¯¸ì—„ì€ í•­ìƒ ì‚¬ìš© ê°€ëŠ¥ (í´ë°± ì§€ì›)
        "memory_usage": "TODO: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰"
    }

# ë¬´ë£Œ ëª¨ë¸ AI ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸ (DialoGPT)
@app.post("/api/chat/free", response_model=ChatResponse)
async def eft_chat_free(request: ChatRequest):
    """
    ë¬´ë£Œ í‹°ì–´ EFT AI ìƒë‹´ ì±„íŒ… (DialoGPT ê¸°ë°˜)
    - í† í° ì œí•œ: 1024 í† í°
    - ê¸°ë³¸ ê°ì • ë¶„ì„ ë° EFT ì¶”ì²œ
    """
    if not ai_engine:
        raise HTTPException(
            status_code=503, 
            detail="AI ëª¨ë¸ì´ ì•„ì§ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        )
    
    try:
        start_time = time.time()
        
        # 1. ê°ì • ë¶„ì„
        emotion_analysis = await emotion_analyzer.analyze(request.message)
        logger.info(f"[FREE] ê°ì • ë¶„ì„: {emotion_analysis}")
        
        # 2. EFT ë§ì¶¤ í”„ë¡¬í”„íŠ¸ ìƒì„±
        eft_prompt = prompt_manager.build_eft_prompt(
            user_message=request.message,
            emotion_state=emotion_analysis,
            conversation_history=request.conversation_history,
            user_profile=request.user_profile
        )
        
        # 3. ë¬´ë£Œ ëª¨ë¸ ì‘ë‹µ ìƒì„± (í† í° ì œí•œ)
        ai_response = await ai_engine.generate_response(
            prompt=eft_prompt,
            max_tokens=min(request.max_tokens or 150, 150),  # ë¬´ë£ŒëŠ” ìµœëŒ€ 150í† í°
            temperature=request.temperature or 0.7
        )
        
        # 4. í›„ì²˜ë¦¬ ë° EFT ì¶”ì²œ
        processed_response = prompt_manager.post_process_response(
            ai_response, emotion_analysis
        )
        
        processing_time = time.time() - start_time
        
        # 5. ì‘ë‹µ ë°˜í™˜
        return ChatResponse(
            response=processed_response["text"],
            emotion_analysis=emotion_analysis,
            eft_recommendations=processed_response["eft_recommendations"],
            suggested_actions=processed_response["suggested_actions"],
            confidence_score=processed_response["confidence"],
            processing_time=processing_time,
            timestamp=datetime.now().isoformat(),
            response_id=f"free_resp_{int(time.time() * 1000)}",
            tier="free"
        )
        
    except Exception as e:
        logger.error(f"ë¬´ë£Œ ì±„íŒ… ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"AI ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )

# ìœ ë£Œ ëª¨ë¸ AI ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸ (Llama-3.1-8B)
@app.post("/api/chat/premium", response_model=ChatResponse)
async def eft_chat_premium(request: ChatRequest):
    """
    í”„ë¦¬ë¯¸ì—„ í‹°ì–´ EFT AI ìƒë‹´ ì±„íŒ… (Llama-3.1-8B ê¸°ë°˜)
    - í† í° ì œí•œ: 4000 í† í°
    - ê³ ê¸‰ ê°ì • ë¶„ì„ ë° ì „ë¬¸ EFT ìƒë‹´
    - ê°œì¸í™”ëœ ë§ì¶¤ ì¶”ì²œ
    """
    # í”„ë¦¬ë¯¸ì—„ ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ ì²´í¬
    active_engine = premium_ai_engine if premium_ai_engine else ai_engine
    
    if not active_engine:
        raise HTTPException(
            status_code=503, 
            detail="AI ëª¨ë¸ì´ ì•„ì§ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        )
    
    # í”„ë¦¬ë¯¸ì—„ ëª¨ë¸ì´ ì—†ìœ¼ë©´ í´ë°± ì•ˆë‚´
    if not premium_ai_engine:
        logger.warning("í”„ë¦¬ë¯¸ì—„ ëª¨ë¸ ì‚¬ìš© ë¶ˆê°€, ë¬´ë£Œ ëª¨ë¸ë¡œ í´ë°±")
    
    try:
        start_time = time.time()
        
        # 1. ê³ ê¸‰ ê°ì • ë¶„ì„
        emotion_analysis = await emotion_analyzer.analyze(request.message)
        logger.info(f"[PREMIUM] ê°ì • ë¶„ì„: {emotion_analysis}")
        
        # 2. ê³ ê¸‰ EFT ë§ì¶¤ í”„ë¡¬í”„íŠ¸ ìƒì„±
        eft_prompt = prompt_manager.build_eft_prompt(
            user_message=request.message,
            emotion_state=emotion_analysis,
            conversation_history=request.conversation_history,
            user_profile=request.user_profile,
            tier="premium"  # í”„ë¦¬ë¯¸ì—„ ì „ìš© í”„ë¡¬í”„íŠ¸
        )
        
        # 3. í”„ë¦¬ë¯¸ì—„ ëª¨ë¸ ì‘ë‹µ ìƒì„± (ë†’ì€ í† í° í•œë„)
        ai_response = await active_engine.generate_response(
            prompt=eft_prompt,
            max_tokens=min(request.max_tokens or 800, 800),  # í”„ë¦¬ë¯¸ì—„ì€ ìµœëŒ€ 800í† í°
            temperature=request.temperature or 0.7
        )
        
        # 4. ê³ ê¸‰ í›„ì²˜ë¦¬ ë° ì „ë¬¸ EFT ì¶”ì²œ
        processed_response = prompt_manager.post_process_response(
            ai_response, emotion_analysis, tier="premium"
        )
        
        processing_time = time.time() - start_time
        
        # 5. ì‘ë‹µ ë°˜í™˜
        return ChatResponse(
            response=processed_response["text"],
            emotion_analysis=emotion_analysis,
            eft_recommendations=processed_response["eft_recommendations"],
            suggested_actions=processed_response["suggested_actions"],
            confidence_score=processed_response["confidence"],
            processing_time=processing_time,
            timestamp=datetime.now().isoformat(),
            response_id=f"premium_resp_{int(time.time() * 1000)}",
            tier="premium"
        )
        
    except Exception as e:
        logger.error(f"í”„ë¦¬ë¯¸ì—„ ì±„íŒ… ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"AI ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )

# ê¸°ì¡´ ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸ (ë¬´ë£Œ ëª¨ë¸ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸)
@app.post("/api/chat", response_model=ChatResponse)
async def eft_chat(request: ChatRequest):
    """
    ê¸°ë³¸ EFT AI ìƒë‹´ ì±„íŒ… (ë¬´ë£Œ ëª¨ë¸ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸)
    í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€
    """
    return await eft_chat_free(request)

# ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… (ê¸´ ì‘ë‹µìš©)
@app.post("/api/chat/stream")
async def eft_chat_stream(request: ChatRequest):
    """ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… (ê¸´ ì‘ë‹µìš©)"""
    if not ai_engine:
        raise HTTPException(status_code=503, detail="AI ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    async def generate_stream():
        try:
            # ê°ì • ë¶„ì„
            emotion_analysis = await emotion_analyzer.analyze(request.message)
            
            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
            async for chunk in ai_engine.generate_stream(
                message=request.message,
                emotion_state=emotion_analysis
            ):
                yield f"data: {json.dumps(chunk, ensure_ascii=False)}\n\n"
                
        except Exception as e:
            error_chunk = {"error": str(e), "type": "generation_error"}
            yield f"data: {json.dumps(error_chunk, ensure_ascii=False)}\n\n"
    
    from fastapi.responses import StreamingResponse
    return StreamingResponse(
        generate_stream(), 
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
        }
    )

# ê°ì • ë¶„ì„ ì „ìš© ì—”ë“œí¬ì¸íŠ¸
@app.post("/api/analyze/emotion")
async def analyze_emotion(request: dict):
    """í…ìŠ¤íŠ¸ ê°ì • ë¶„ì„"""
    if not emotion_analyzer:
        raise HTTPException(status_code=503, detail="ê°ì • ë¶„ì„ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    text = request.get("text", "")
    if not text:
        raise HTTPException(status_code=400, detail="ë¶„ì„í•  í…ìŠ¤íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    
    analysis = await emotion_analyzer.analyze(text)
    return {
        "text": text,
        "emotion_analysis": analysis,
        "timestamp": datetime.now().isoformat()
    }

# EFT ê¸°ë²• ì¶”ì²œ ì—”ë“œí¬ì¸íŠ¸
@app.post("/api/recommend/eft")
async def recommend_eft_technique(request: dict):
    """ê°ì • ìƒíƒœ ê¸°ë°˜ EFT ê¸°ë²• ì¶”ì²œ"""
    if not prompt_manager:
        raise HTTPException(status_code=503, detail="í”„ë¡¬í”„íŠ¸ ì‹œìŠ¤í…œì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    emotion_state = request.get("emotion_state")
    if not emotion_state:
        raise HTTPException(status_code=400, detail="ê°ì • ìƒíƒœ ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    
    recommendations = prompt_manager.recommend_eft_techniques(emotion_state)
    return {
        "emotion_state": emotion_state,
        "recommendations": recommendations,
        "timestamp": datetime.now().isoformat()
    }

# ëª¨ë¸ ì„±ëŠ¥ í†µê³„
@app.get("/api/stats")
async def get_model_stats():
    """ëª¨ë¸ ì„±ëŠ¥ ë° ì‚¬ìš© í†µê³„"""
    if not ai_engine:
        return {"error": "AI ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}
    
    stats = await ai_engine.get_performance_stats()
    return {
        "model_stats": stats,
        "server_uptime": time.time(),
        "total_requests": "TODO: ìš”ì²­ ìˆ˜ ì¶”ì ",
        "average_response_time": "TODO: í‰ê·  ì‘ë‹µ ì‹œê°„"
    }

# Enhanced vLLM upstream health check endpoint  
@app.get("/health/upstreams")
async def health_upstreams(req: Request, x_admin_token: Optional[str] = Header(None)):
    """vLLM upstream ì„œë²„ë“¤ì˜ ìƒíƒœë¥¼ ì²´í¬í•©ë‹ˆë‹¤ (ìš´ì˜ì—ì„œëŠ” ë‚´ë¶€ë§/ê´€ë¦¬ì ì „ìš©)"""
    
    # ìš´ì˜ í™˜ê²½ì—ì„œ ë³´ì•ˆ ì²´í¬
    if not settings.DEBUG:
        # í† í° ìš°ì„  ì²´í¬
        if settings.ADMIN_API_KEY and x_admin_token != settings.ADMIN_API_KEY:
            raise HTTPException(status_code=403, detail="forbidden")
        # í† í°ì´ ì—†ìœ¼ë©´ ë‚´ë¶€ IPë§Œ
        client = (req.client.host if req.client else "")
        if client not in settings.INTERNAL_NETWORKS:
            raise HTTPException(status_code=403, detail="forbidden")
    import httpx
    upstreams = {}
    
    for engine_key, config in settings.FREE_ENGINES.items():
        port = config["port"]
        model = config["model"] 
        base_url = f"http://127.0.0.1:{port}"
        
        try:
            async with httpx.AsyncClient(timeout=5.0) as client:
                start = time.time()
                r = await client.get(f"{base_url}/v1/models")
                latency = (time.time() - start) * 1000  # ms
                
                if r.status_code == 200:
                    models = r.json().get("data", [])
                    available_models = [m.get("id") for m in models]
                    upstreams[engine_key] = {
                        "status": "healthy",
                        "url": base_url,
                        "expected_model": model,
                        "available_models": available_models,
                        "latency_ms": round(latency, 2),
                        "error": None
                    }
                else:
                    upstreams[engine_key] = {
                        "status": "unhealthy", 
                        "url": base_url,
                        "expected_model": model,
                        "error": f"HTTP {r.status_code}: {r.text[:200]}"
                    }
        except Exception as e:
            upstreams[engine_key] = {
                "status": "unreachable",
                "url": base_url, 
                "expected_model": model,
                "error": str(e)
            }
    
    # ì „ì²´ ìƒíƒœ ê²°ì •
    all_statuses = [u["status"] for u in upstreams.values()]
    overall_status = "healthy" if "healthy" in all_statuses else "degraded" if any(s != "unreachable" for s in all_statuses) else "unhealthy"
    
    return {
        "overall_status": overall_status,
        "upstreams": upstreams,
        "timestamp": datetime.now().isoformat(),
        "strategy": settings.AB_TEST_STRATEGY
    }

# A/B í…ŒìŠ¤íŠ¸ìš© ì±„íŒ… ì™„ì„± ì—”ë“œí¬ì¸íŠ¸ (ê°•í™”)
class ChatProxyRequest(BaseModel):
    """ì±„íŒ… í”„ë¡ì‹œ ìš”ì²­ ëª¨ë¸"""
    message: str = Field(..., min_length=1, max_length=4000, description="ì‚¬ìš©ì ë©”ì‹œì§€")
    temperature: Optional[float] = Field(default=0.7, ge=0.0, le=2.0, description="ì°½ì˜ì„± ìˆ˜ì¤€")
    max_tokens: Optional[int] = Field(default=512, ge=1, le=2000, description="ìµœëŒ€ í† í° ìˆ˜")
    model: Optional[str] = Field(default=None, description="ìš”ì²­ ëª¨ë¸ëª… (ì„ íƒì‚¬í•­)")
    
# í´ë°± ë¡œì§ì„ ìœ„í•œ ë„ìš°ë¯¸ í•¨ìˆ˜
def other_engine_key(cur_key: str) -> Optional[str]:
    """í˜„ì¬ ì—”ì§„ì„ ì œì™¸í•œ ë‹¤ë¥¸ ì—”ì§„ ë°˜í™˜"""
    keys = list(settings.FREE_ENGINES.keys())
    if len(keys) < 2: 
        return None
    for k in keys:
        if k != cur_key:
            return k
    return None

@app.post("/api/chat/completion")
async def completion(request: ChatProxyRequest, req: Request):
    """A/B í…ŒìŠ¤íŠ¸ìš© ì±„íŒ… ì™„ì„± ì—”ë“œí¬ì¸íŠ¸ (ê°•í™” + í´ë°±)"""
    import httpx
    
    # ìƒê´€ê´€ê³„ ID ì¶”ê°€
    correlation_id = getattr(req.state, 'correlation_id', 'unknown')
    logger.info(f"[{correlation_id}] ì±„íŒ… ìš”ì²­ ì‹œì‘: {request.message[:50]}...")
    
    # ë¬´ë£Œ í‹°ì–´ -> A/B ì—”ì§„ìœ¼ë¡œ í”„ë¡ì‹œ
    if hasattr(req.state, 'free_engine') and req.state.free_engine:
        engine = req.state.free_engine
        base = f"http://127.0.0.1:{engine['port']}/v1"
        
        # vLLM(OpenAI í˜¸í™˜) chat.completions
        payload = {
            "model": request.model or engine["model"],
            "messages": [
                {"role": "system", "content": "You are a helpful EFT counselor assistant specialized in Korean emotional support."},
                {"role": "user", "content": request.message},
            ],
            "temperature": request.temperature,
            "max_tokens": request.max_tokens,
        }
        
        # ê¸°ë³¸ ì—”ì§„ ì‹œë„ + í´ë°± ë¡œì§
        timeout_config = httpx.Timeout(connect=10.0, read=120.0, write=10.0, pool=10.0)
        
        async def try_engine(engine_key: str, engine_config: dict, is_fallback: bool = False):
            base_url = f"http://127.0.0.1:{engine_config['port']}/v1"
            try:
                async with httpx.AsyncClient(timeout=timeout_config) as client:
                    start_time = time.time()
                    r = await client.post(f"{base_url}/chat/completions", json=payload)
                    processing_time = time.time() - start_time
                    
                if r.status_code >= 400:
                    raise httpx.HTTPStatusError(f"HTTP {r.status_code}", request=r.request, response=r)
                    
                data = r.json()
                content = data["choices"][0]["message"]["content"]
                
                logger.info(f"[{correlation_id}] {'Fallback ' if is_fallback else ''}ì„±ê³µ: {engine_key} ({processing_time:.3f}s)")
                
                return {
                    "tier": "free",
                    "engine": engine_key,
                    "model": engine_config["model"],
                    "reply": content,
                    "processing_time": round(processing_time, 3),
                    "timestamp": datetime.now().isoformat(),
                    "fallback_used": is_fallback
                }
            except Exception as e:
                logger.error(f"[{correlation_id}] ì—”ì§„ {engine_key} ì‹¤íŒ¨: {str(e)[:200]}")
                raise e
        
        try:
            # 1ì°¨ ì‹œë„: ê¸°ë³¸ ì—”ì§„
            return await try_engine(req.state.free_engine_key, engine, False)
            
        except Exception as primary_error:
            # í´ë°± ì‹œë„
            alt_key = other_engine_key(req.state.free_engine_key)
            if alt_key and alt_key in settings.FREE_ENGINES:
                logger.warning(f"[{correlation_id}] ê¸°ë³¸ ì—”ì§„ ì‹¤íŒ¨, í´ë°± ì‹œë„: {req.state.free_engine_key} -> {alt_key}")
                try:
                    return await try_engine(alt_key, settings.FREE_ENGINES[alt_key], True)
                except Exception as fallback_error:
                    logger.error(f"[{correlation_id}] í´ë°±ë„ ì‹¤íŒ¨: {fallback_error}")
                    
            # ëª¨ë“  ì—”ì§„ ì‹¤íŒ¨ ì‹œ ì›ë˜ ì—ëŸ¬ ë°˜í™˜
            if isinstance(primary_error, httpx.TimeoutException):
                raise HTTPException(status_code=504, detail=f"vLLM ì„œë²„ ì‘ë‹µ ì‹œê°„ ì´ˆê³¼: {engine['model']}")
            elif isinstance(primary_error, httpx.ConnectError):
                raise HTTPException(status_code=503, detail=f"vLLM ì„œë²„ ì—°ê²° ë¶ˆê°€: {engine['model']} (í¬íŠ¸ {engine['port']})")
            else:
                raise HTTPException(status_code=500, detail=f"vLLM ì„œë²„ ì˜¤ë¥˜: {str(primary_error)}")
            
        # ìœ„ì˜ try-except ë¡œì§ì—ì„œ ì²˜ë¦¬ë¨

    # í”„ë¦¬ë¯¸ì—„/ì—”í„°í”„ë¼ì´ì¦ˆ: ê¸°ì¡´ ê²½ë¡œë¡œ í´ë°±
    try:
        # ChatRequestë¡œ ë³€í™˜í•˜ì—¬ ê¸°ì¡´ í”„ë¦¬ë¯¸ì—„ ì—”ë“œí¬ì¸íŠ¸ í˜¸ì¶œ
        chat_req = ChatRequest(
            message=request.message,
            temperature=request.temperature,
            max_tokens=request.max_tokens
        )
        
        response = await eft_chat_premium(chat_req)
        return {
            "tier": response.tier,
            "model": settings.PREMIUM_TIER_MODEL,
            "reply": response.response,
            "processing_time": response.processing_time,
            "timestamp": response.timestamp
        }
        
    except Exception as e:
        logger.error(f"í”„ë¦¬ë¯¸ì—„ ëª¨ë¸ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"AI ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {str(e)}")

if __name__ == "__main__":
    import uvicorn
    
    # ê°œë°œ ì„œë²„ ì‹¤í–‰
    uvicorn.run(
        "main:app",
        host=settings.HOST,
        port=settings.PORT,
        reload=settings.DEBUG,
        log_level="info" if settings.DEBUG else "warning"
    )